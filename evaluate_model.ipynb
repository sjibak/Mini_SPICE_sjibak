{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83658c30-0ab1-405c-a596-3b6fb5aefa57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937d273-9d48-4ebf-9373-62df60e11032",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from data_files.dataset import CQT_Dataset_test\n",
    "import torch\n",
    "from utils.model import Spice_model\n",
    "import numpy as np\n",
    "from utils.calibration import Calibrator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ff30f-340d-4be4-ae0d-11663586c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdb_test_data = pd.read_pickle(\"./data_test/MedleyDB.pkl\") \n",
    "mir_test_data = pd.read_pickle(\"./data_test/MIR1k.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4869610-1f48-4538-ae3e-b495cc362cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mir_test_data\n",
    "data_MIR = mir_test_data.to_numpy()\n",
    "data_MDB = mdb_test_data.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b261f90-a57b-41cb-886a-724be2215d72",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752037aa-d909-4584-82a9-a078d550f093",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Model and Calibation (Plots Calibration Estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c5139f-5788-40ae-8a62-34fc00457646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model to be evaluated\n",
    "model = Spice_model([1, 64, 128, 256, 512, 512, 512], [512, 512, 512, 256, 128, 64, 1], [True, True, True, True, True, True])\n",
    "checkpoint = torch.load('./rev_1k_checkpoints/MIR_minispice.ckp', 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d45945d-6794-4e7c-a32e-7533cb9eff4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cal1 = Calibrator(model, 1000, 110, 440)\n",
    "PT_OFFSET, PT_SLOPE = cal1.get_values()\n",
    "data, A, B= cal1.get_data()\n",
    "\n",
    "\n",
    "# Uncoment For Plots\n",
    "# x = np.linspace(0, 1, 5)\n",
    "# y = PT_OFFSET + PT_SLOPE*x\n",
    "# plt.scatter(B.squeeze(),A[:,1])\n",
    "# plt.xlim(30, 70)\n",
    "# plt.ylim(0.3, 0.55)\n",
    "# plt.plot(y, x, 'r',linestyle='--',label='Line: Estimated Parameters')\n",
    "# plt.xlabel('Pith Diff from fmin(10Hz) [Semitones]')\n",
    "# plt.ylabel('Pitch Head Output')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01752fa4-ff7d-4433-a7aa-04b2fb513be7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def output2hz(pitch_output):\n",
    "  # Constants taken from https://tfhub.dev/google/spice/2\n",
    "  # PT_OFFSET = 75.06398400431725\n",
    "  # PT_SLOPE = -43.583755096345676\n",
    "  FMIN = 10.0    \n",
    "  BINS_PER_OCTAVE = 12.0  \n",
    "  cqt_bin = pitch_output * PT_SLOPE + PT_OFFSET;\n",
    "  return FMIN*2**(cqt_bin/BINS_PER_OCTAVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37bf0f6-2c56-407c-b521-109cebed0061",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Whole Batch RPA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9b234-838d-4ec7-8f06-a5a16d4abc0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mir_eval\n",
    "from tqdm import tqdm\n",
    "label = []\n",
    "yt_hat = []\n",
    "voice = []\n",
    "# data_part\n",
    "for row in tqdm(data_MDB):\n",
    "    pitch_h1,conf_h1,x_hat1 = model(torch.from_numpy(row[1:129].reshape(1,128)).float())\n",
    "    yt_hat.append(pitch_h1.detach().numpy())\n",
    "    voice.append(row[-2])\n",
    "    label.append(row[-1])\n",
    "y_hat = np.apply_along_axis(output2hz,0,yt_hat)\n",
    "y_hat_cent = np.apply_along_axis(mir_eval.melody.hz2cents, 0, y_hat)\n",
    "label_cent = np.apply_along_axis(mir_eval.melody.hz2cents, 0, label)\n",
    "# voice = self.test_data[:,-2:-1]\n",
    "voice = np.array(voice).reshape(len(voice),1)\n",
    "label_cent = np.array(label_cent).reshape(len(label_cent),1)\n",
    "y_hat_cent= np.array(y_hat_cent).reshape(len(y_hat_cent),1)\n",
    "rpa = mir_eval.melody.raw_pitch_accuracy(voice, label_cent, voice, y_hat_cent, cent_tolerance=50)\n",
    "rpa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c720f14-2eb2-4af6-b9a8-83b27820bdbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Results - RPA vs M Samples on MIR\n",
    "0.23760423855905596, M=2\n",
    "<br>\n",
    "0.3555553249233292, M=3\n",
    "<br>\n",
    "0.02949555542870783, M=5\n",
    "<br>\n",
    "0.23164700815227263 , M = 10\n",
    "<br>\n",
    "0.06791138879231165 , M = 20\n",
    "<br>\n",
    "0.12203500651247749, M= 50\n",
    "<br>\n",
    "## Results - RPA \n",
    "RPA - MIR_noconf.ckp = 0.02232404635015853\n",
    "<br>\n",
    "RPA - MIR_ninispice.ckp = MIR 0.07354688724332796 | MDB 0.08407682146123362\n",
    "<br>\n",
    "RPA - wo-recon.ckp = MIR 0.4413020803603398, 0.29013476417566564| MDB 0.06933577210988043\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c721c5f-a11d-407b-b420-1ff9016cdc5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(0.4413020803603398+0.29013476417566564)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6285e553-600e-44c3-9f12-f580e0f36914",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Calibration on SPICE (Plots Calibration Estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c241d-4de7-4ece-8903-cee29b4bbd12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from librosa import display as librosadisplay\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "import math\n",
    "import statistics\n",
    "import sys\n",
    "\n",
    "from IPython.display import Audio, Javascript\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from base64 import b64decode\n",
    "\n",
    "import music21\n",
    "from pydub import AudioSegment\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "print(\"tensorflow: %s\" % tf.__version__)\n",
    "#print(\"librosa: %s\" % librosa.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d2a46-4886-456c-9797-984447c9702d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env http_proxy=http://proxy:80\n",
    "%env https_proxy=http://proxy:80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc577ce-9e6b-4166-a8ca-928c0a5c8205",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the SPICE model is easy:\n",
    "model_hub = hub.load(\"https://tfhub.dev/google/spice/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b377b1b-5b4b-4bd0-98ec-d4a81bc737e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.calibration2 import Calibrator_SPICE\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4eda269-a2e7-46fb-b4e4-531485d570f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Calibrator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cal \u001b[38;5;241m=\u001b[39m \u001b[43mCalibrator\u001b[49m(model_hub, \u001b[38;5;28;01mFalse\u001b[39;00m,\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      2\u001b[0m PT_OFFSET, PT_SLOPE \u001b[38;5;241m=\u001b[39m cal\u001b[38;5;241m.\u001b[39mget_values()\n\u001b[1;32m      3\u001b[0m A, B\u001b[38;5;241m=\u001b[39m cal\u001b[38;5;241m.\u001b[39mget_data()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Calibrator' is not defined"
     ]
    }
   ],
   "source": [
    "cal = Calibrator_SPICE(model_hub, False,1000)\n",
    "PT_OFFSET, PT_SLOPE = cal.get_values()\n",
    "A, B= cal.get_data()\n",
    "\n",
    "# for plotting\n",
    "# x = np.linspace(0, 1, 5)\n",
    "# # Define the equation\n",
    "# y = PT_OFFSET + PT_SLOPE*x\n",
    "# plt.scatter(B.squeeze(),A[:,1])\n",
    "# plt.xlim(40, 70)\n",
    "# plt.ylim(0.21, 0.64)\n",
    "# plt.plot(y, x, 'r',linestyle='--',label='Line: Estimated Parameters')\n",
    "# plt.xlabel('Pith Diff from fmin(10Hz) [Semitones]')\n",
    "# plt.ylabel('Pitch Head Output')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a37d227-97d0-4234-a78e-717fe10783ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "# This part extra, for SPICE evaluaton refer to Mini_SPICE notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42416d09-1ad8-4ad0-8173-6e0f78d2b261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "label = []\n",
    "yt_hat = []\n",
    "voice = []\n",
    "# pitch_h1,conf_h1,x_hat1 = model_mdb(torch.from_numpy(data_np[:1,1:129].reshape(1,128)).float())\n",
    "# model_mdb(torch.from_numpy(row[1:129].reshape(1,128)).float())\n",
    "for row in tqdm(data_MDB):\n",
    "    pitch_h1,conf_h1,x_hat1 = model_mdb(torch.from_numpy(row[1:129].reshape(1,128)).float())\n",
    "    yt_hat.append(pitch_h1.detach().numpy())\n",
    "    voice.append(row[-2])\n",
    "    label.append(row[-1])\n",
    "# y_hat = np.apply_along_axis(output2hz, 0, pitch_h1.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0087212-0251-46b6-bacb-f96dc9b2e31c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mir_eval\n",
    "y_hat = np.apply_along_axis(output2hz,0,yt_hat)\n",
    "y_hat_cent = np.apply_along_axis(mir_eval.melody.hz2cents, 0, y_hat)\n",
    "label_cent = np.apply_along_axis(mir_eval.melody.hz2cents, 0, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a5ea2-ad0c-4294-ab72-627a473f696a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "voice = np.array(voice).reshape(len(voice),1)\n",
    "label_cent = np.array(label_cent).reshape(len(label_cent),1)\n",
    "y_hat_cent= np.array(y_hat_cent).reshape(len(y_hat_cent),1)\n",
    "# y_hat_cent\n",
    "rpa = mir_eval.melody.raw_pitch_accuracy(voice, label_cent, voice, y_hat_cent, cent_tolerance=50)\n",
    "rpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad0412-33b1-4e1f-b333-e5b4a507fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_h,conf_h,x_hat = model_mdb(torch.randn((10,128)))\n",
    "y_hat = np.apply_along_axis(output2hz, 0, pitch_h.detach().numpy())\n",
    "y_hat_cent = np.apply_along_axis(mir_eval.melody.hz2cents, 0, y_hat)\n",
    "y_hat_voice = np.random.randint(2, size=y_hat.shape)\n",
    "y = np.random.uniform(low=40, high=65, size=(10,1)) \n",
    "y_cent = np.apply_along_axis(mir_eval.melody.hz2cents, 0, y)\n",
    "y_voice = np.random.randint(2, size=y.shape)\n",
    "raw_pitch_accuracy = mir_eval.melody.raw_pitch_accuracy(y_voice, y_cent, y_hat_voice, y_hat_cent)\n",
    "raw_pitch_accuracy\n",
    "y_voice.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd33afb-cde4-4e1e-9543-e1f02fba67ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Skip this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a29a9-6d51-4820-8047-4946138ffbf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "mir_test_batches = DataLoader(CQT_Dataset_test(data=mir_test_data, mode='test'), batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4263eb6-3110-45e2-a610-a54d58114008",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for inputs, targets in enumerate(mir_test_batches):\n",
    "    int_shift = targets[1]\n",
    "    inp1 = targets[2]\n",
    "    inp2 = targets[3]\n",
    "    label = np.vstack((targets[4].detach().numpy(),targets[5].detach().numpy()))\n",
    "    if inputs == 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d8fd1e-7b59-48a3-98b4-11c7540fe3d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def output2hz(pitch_output):\n",
    "  # Constants taken from https://tfhub.dev/google/spice/2\n",
    "  PT_OFFSET = 10.806732248081362\n",
    "  PT_SLOPE = -56.44811563764634\n",
    "  FMIN = 10.0    #why is it 10, not 110?\n",
    "  BINS_PER_OCTAVE = 12.0  \n",
    "  cqt_bin = pitch_output * PT_SLOPE + PT_OFFSET;\n",
    "  return FMIN * 2.0 ** (1.0 * cqt_bin / BINS_PER_OCTAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515888f9-17db-4183-bfb3-57870ce1b1c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "1 / (24 * np.log2(666.664939769901 /  66.9456331525636256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e921f-827e-4708-9822-90b61be00fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mir_eval\n",
    "from tqdm import tqdm\n",
    "\n",
    "def rpa_on_dataset(model, batches, sigma=0.0125657):\n",
    "    pred_pitch = np.array([])\n",
    "    pred_pitch_cent = np.array([])\n",
    "    pred_pitch_voicing = np.array([])\n",
    "    lab_pitch_cent = np.array([])\n",
    "    lab_pitch_voicing = np.array([])\n",
    "    for inputs, targets in enumerate(tqdm(batches)):\n",
    "        int_shift = targets[1].detach().numpy()\n",
    "        inp1 = targets[2]\n",
    "        inp2 = targets[3]\n",
    "        label_voice = targets[4].detach().numpy()\n",
    "        label_f0 = targets[5].detach().numpy()\n",
    "        pitch_h1,conf_h1,x_hat1 = model(inp1.float())\n",
    "        pitch_h2,conf_h2,x_hat2 = model(inp2.float())\n",
    "        abs_pitch1 = np.apply_along_axis(output2hz, 0, pitch_h1.detach().numpy())\n",
    "        abs_pitch2 = np.apply_along_axis(output2hz, 0, pitch_h2.detach().numpy())\n",
    "        # abs_pitch_cent1 = np.apply_along_axis(mir_eval.melody.hz2cents, 0, abs_pitch1))\n",
    "        # abs_pitch_cent2 = np.apply_along_axis(mir_eval.melody.hz2cents, 0, abs_pitch2))\n",
    "        pitch_diff =  np.abs((abs_pitch1-abs_pitch2) - (sigma*int_shift).reshape(int_shift.shape[0],1))\n",
    "        # pitch_diff_cent =  np.abs(abs_pitch_cent1-abs_pitch_cent2)\n",
    "        # pitch_diff[pitch_diff>0.5] #not even a sigle one even without semitone implementation\n",
    "        \n",
    "        ## implemented on average pitch\n",
    "        pred_pitch_batch = np.mean([abs_pitch1, abs_pitch2], axis=0)\n",
    "        pred_pitch = np.append(pred_pitch, pred_pitch_batch)\n",
    "        temp = np.apply_along_axis(mir_eval.melody.hz2cents, 0, pred_pitch_batch)\n",
    "        pred_pitch_cent = np.append(pred_pitch_cent, temp)\n",
    "        pred_pitch_voicing = np.append(pred_pitch_voicing,label_voice) #replace with conf head\n",
    "        lab_pitch_cent = np.append(lab_pitch_cent,np.apply_along_axis(mir_eval.melody.hz2cents, 0, label_f0))\n",
    "        lab_pitch_voicing = np.append(lab_pitch_voicing,label_voice) \n",
    "        if inputs == 5:\n",
    "            break\n",
    "        print(\"RPA : {} after batch {}\".format(mir_eval.melody.raw_pitch_accuracy(lab_pitch_voicing, lab_pitch_cent, pred_pitch_voicing, pred_pitch_cent), inputs))\n",
    "    raw_pitch_accuracy = mir_eval.melody.raw_pitch_accuracy(lab_pitch_voicing, lab_pitch_cent, pred_pitch_voicing, pred_pitch_cent)\n",
    "    return raw_pitch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8939a2a5-dee4-43a3-8dc9-f747b929bb59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rpa_on_dataset(model, mir_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eccbb3-b6e3-41cf-9c3a-e7636c8d91b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.apply_along_axis(output2hz, 0, pitch_h.detach().numpy())\n",
    "y_hat_cent = np.apply_along_axis(mir_eval.melody.hz2cents, 0, y_hat)\n",
    "y_hat_voice = np.random.randint(2, size=y_hat.shape)\n",
    "y = np.random.uniform(low=40, high=65, size=(10,1)) #extrapolate back to time\n",
    "y_cent = np.apply_along_axis(mir_eval.melody.hz2cents, 0, y)\n",
    "y_voice = np.random.randint(2, size=y.shape)\n",
    "raw_pitch_accuracy = mir_eval.melody.raw_pitch_accuracy(y_voice, y_cent, y_hat_voice, y_hat_cent)\n",
    "raw_pitch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50256fe-6d37-4280-99d9-3348455e159d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pitch_h1,conf_h1,x_hat1 = model(inp1.float())\n",
    "pitch_h2,conf_h2,x_hat2 = model(inp2.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b1e13-728a-4d7d-9df5-1c2ae12220b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_hat1 = np.apply_along_axis(output2hz, 0, pitch_h1.detach().numpy())\n",
    "y_hat2 = np.apply_along_axis(output2hz, 0, pitch_h2.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019312cf-af38-48c4-8462-40040c0143ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"{}\\n{}\\n{}\\n{}\".format(y_hat1,y_hat2,label,int_shift))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cb2c65-bee1-4a63-9311-d1d048bce426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spice",
   "language": "python",
   "name": "spice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
